{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics as st\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import matplotlib.dates as md\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from stldecompose import decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import logging\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Stationarity function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationarity(timeseries):\n",
    "    \"\"\"Performs  Dickey-Fuller test to test stationairty\"\"\"\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print(dfoutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchor Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchor(signal, weight):\n",
    "    \"\"\"Data Smooothing\"\"\"\n",
    "    buffer = []\n",
    "    last = signal[0]\n",
    "    for i in signal:\n",
    "        smoothed_val = last * weight + (1 - weight) * i\n",
    "        buffer.append(smoothed_val)\n",
    "        last = smoothed_val\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abline Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abline(slope, intercept):\n",
    "    \"\"\"Plots a line from slope and intercept\"\"\"\n",
    "    axes = plt.gca()\n",
    "    x_vals = np.array(axes.get_xlim())\n",
    "    y_vals = intercept + slope * x_vals\n",
    "    plt.plot(x_vals, y_vals, '--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strongest Trend Period Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strongest_trend_period(ts, start, end):\n",
    "    \"\"\"Trend Detection, ts shape(x, )\"\"\"\n",
    "    trend_strengths = []\n",
    "    for i in range(start, end):\n",
    "        decomposition = decompose(ts, period = i)\n",
    "        var_residual = np.power(st.stdev(decomposition.resid), 2)\n",
    "        var_trend_residual = np.power(st.stdev(anchor(decomposition.trend, 0.9) + decomposition.resid), 2)\n",
    "        trend_strength = max(0, (1 - var_residual/var_trend_residual))\n",
    "        trend_strengths.append(trend_strength)\n",
    "    d = dict()\n",
    "    d[\"index\"] = trend_strengths.index(max(trend_strengths))\n",
    "    d[\"period\"] = d[\"index\"] + 1\n",
    "    d[\"trend_strength\"] = max(trend_strengths)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strongest Seasonal Period Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strongest_seasonal_period(ts, start, end):\n",
    "    \"\"\"Period Detecion, tx shape (x, )\"\"\"\n",
    "    seasonal_strengths = []\n",
    "    for i in range(start, end):\n",
    "        decomposition = decompose(ts, period = i)\n",
    "        var_residual = np.power(st.stdev(decomposition.resid), 2)\n",
    "        var_seasonal_residual = np.power(st.stdev(decomposition.seasonal + decomposition.resid), 2)\n",
    "        seasonality_strength = max(0, (1 - var_residual/var_seasonal_residual))\n",
    "        seasonal_strengths.append(seasonality_strength)\n",
    "    d = dict()\n",
    "    d[\"index\"] = seasonal_strengths.index(max(seasonal_strengths))\n",
    "    d[\"period\"] = d[\"index\"] + 1\n",
    "    d[\"seasonality_strength\"] = max(seasonal_strengths)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Trend Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_trend(ts, p):\n",
    "    \"\"\"Remove Trend, ts shape (x, )\"\"\"\n",
    "    decomposition = decompose(ts, period = p)\n",
    "    detrended = ts - decomposition.trend\n",
    "    d = dict()\n",
    "    d[\"detrended\"] = detrended.reshape(detrended.shape[0], 1)\n",
    "    d[\"trend\"] = decomposition.trend.reshape(decomposition.trend.shape[0], 1)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearity Score Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.formula.api as smf\n",
    "def linearity_score(ts):\n",
    "    \"\"\"Linarity Score Detection\"\"\"\n",
    "    x = [e for e in range(0, ts.shape[0])]\n",
    "    x = np.array(x)\n",
    "    trend_train_df = pd.DataFrame({'time': x, 'trend':ts})\n",
    "    model_trend_train = smf.ols('time ~ trend', data = trend_train_df).fit()\n",
    "    return model_trend_train.rsquared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Definition (-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(ts):\n",
    "    \"\"\"Scaling data, -1 to 1, ts shape (x, )\"\"\"\n",
    "    scaler_ts = MinMaxScaler(feature_range = (-1, 1))\n",
    "    scaler_ts = scaler_ts.fit(ts.values.reshape(-1, 1))\n",
    "    ts_scaled = scaler_ts.transform(ts.values.reshape(-1, 1))\n",
    "    ts_scaled = pd.DataFrame(ts_scaled)\n",
    "    d = dict()\n",
    "    d[\"scaler\"] = scaler_ts\n",
    "    d[\"scaled\"] = ts_scaled #(x, 1)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Reg Definction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_reg(train, start, end):\n",
    "    \"\"\"Performs Linear Regression, train shape (x, )\"\"\"\n",
    "    x = [e for e in range(0, train.shape[0])]\n",
    "    x = np.array(x)\n",
    "    x_test = [e for e in range(start, end)]\n",
    "    x_test = np.array(x_test)\n",
    "    linearRegressor = LinearRegression()\n",
    "    linearRegressor.fit(x.reshape(-1, 1), train.reshape(-1, 1))\n",
    "    # model_trend_train.predict(z)\n",
    "    pred = linearRegressor.predict(x_test.reshape(-1, 1))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration for Figure Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 20\n",
    "fig_size[1] = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation By ARMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARMA example\n",
    "# Data creation, random data is created, ARMA is applied, its fitted values are usede as a TS\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "import random\n",
    "\n",
    "data = [random.randint(1, 100) for x in range(0, 100000)]\n",
    "model = ARMA(data, order=(0, 1))\n",
    "model_fit = model.fit(disp=False)\n",
    "plt.plot(data[:1000])\n",
    "test_stationarity(pd.Series(model_fit.fittedvalues))\n",
    "Test Statistic is smaller than vritical value => TS is stationary \n",
    "ts = model_fit.fittedvalues\n",
    "print(type(ts))\n",
    "plt.plot(ts[:100])\n",
    "\n",
    "np.savetxt(\"stationary-arma-ts.csv\", ts, delimiter=\",\", header = \"Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trendy Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "\n",
    "x = [e for e in range(0, 18000)]\n",
    "noise = [5*random.random() for i in range(0, 18000)]\n",
    "y = []\n",
    "for j in range(len(x)):\n",
    "    y.append(x[j])\n",
    "y = np.array(y)\n",
    "data = y + noise\n",
    "plt.plot(data[:50])\n",
    "print(data)\n",
    "\n",
    "np.savetxt(\"trendy-ts-18000.csv\", data, delimiter=\",\", header = \"Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Periodic Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "\n",
    "x = [e for e in range(0, 1300)]\n",
    "noise = [random.random() for i in range(0, 1300)]\n",
    "x = np.arange(0, 195, 0.15);\n",
    "y = np.sin(x)\n",
    "scaler_y = MinMaxScaler(feature_range = (-1, 1))\n",
    "scaler_y = scaler_y.fit(y.reshape(-1, 1))\n",
    "y_scaled = scaler_y.transform(ts.values.reshape(-1, 1))\n",
    "y = y_scaled.flatten()\n",
    "data = y + noise\n",
    "plt.plot(data[:200])\n",
    "\n",
    "np.savetxt(\"seasonal-ts-sin.csv\", data, delimiter=\",\", header = \"Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stationry-Arma-Ts\n",
    "#Trendy-Ts\n",
    "#Seasonal-Ts\n",
    "df = pd.read_csv(\"trendy-ts-18000.csv\")\n",
    "ts = df[\"# Data\"]\n",
    "ts = ts[:18000]\n",
    "print(df['# Data'].describe())\n",
    "print(\"\\n\")\n",
    "print(\"Shape of TS: \" + str(ts.shape))\n",
    "print(\"Type of TS: \" + str(type(ts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling data to (-1, 1) range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_scaler = scale(ts)\n",
    "ts_scaled = ts_scaler[\"scaled\"]\n",
    "batch_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd, nd\n",
    "from mxnet.gluon import nn, rnn\n",
    "\n",
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the data into train (70%) and test (30%) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data_size = ts_scaled.shape[0]\n",
    "train_data_size = int((whole_data_size*70)/100)\n",
    "test_data_size = int((whole_data_size*30)/100)\n",
    "\n",
    "ts_scaled = ts_scaled.values\n",
    "train = ts_scaled[:train_data_size]\n",
    "test = ts_scaled[train_data_size:]\n",
    "\n",
    "print(\"Train set shape: \" + str(train.shape))\n",
    "print(\"Test set shape: \" + str(test.shape))\n",
    "print(\"\\n\")\n",
    "\n",
    "train_sd = st.stdev(train.reshape(train_data_size, ))\n",
    "train_mean = st.mean(train.reshape(train_data_size, ))\n",
    "print(\"Train SD: \" + str(train_sd))\n",
    "print(\"Train Mean: \" + str(train_mean))\n",
    "\n",
    "ts_test = test\n",
    "ts_train = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend and Seasonality\n",
    "##### Scaling for Trend and Detrended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Train\n",
    "str_trd_train = strongest_trend_period(ts_train.reshape(ts_train.shape[0], ), 1, 25)\n",
    "str_trd_train[\"period\"]\n",
    "\n",
    "detrended_train = remove_trend(ts_train.reshape(ts_train.shape[0], ), str_trd_train[\"period\"])\n",
    "if(linearity_score(detrended_train[\"trend\"].reshape(detrended_train[\"trend\"].shape[0], )) >= 0.8):\n",
    "    ts_train = detrended_train[\"detrended\"]\n",
    "    ts_detrended_train_scaler = scale(pd.Series(ts_train.reshape(ts_train.shape[0], )))\n",
    "    ts_train = ts_detrended_train_scaler[\"scaled\"].values\n",
    "\n",
    "str_period_train = strongest_seasonal_period(ts_train.reshape(ts_train.shape[0], ), 1, 25)\n",
    "if(str_period_train[\"seasonality_strength\"] >= 0.8):\n",
    "    batch_size = str_period_train[\"period\"]\n",
    "\n",
    "#For Test\n",
    "\n",
    "str_trd_test = strongest_trend_period(ts_test.reshape(ts_test.shape[0], ), 1, 25)\n",
    "str_trd_test[\"period\"]\n",
    "detrended_test = remove_trend(ts_test.reshape(ts_test.shape[0], ), str_trd_test[\"period\"])\n",
    "\n",
    "if(linearity_score(detrended_test[\"trend\"].reshape(detrended_test[\"trend\"].shape[0], )) >= 0.8):\n",
    "    ts_test = detrended_test[\"detrended\"]\n",
    "    ts_detrended_test_scaler = scale(pd.Series(ts_test.reshape(ts_test.shape[0], )))\n",
    "    ts_test = ts_detrended_test_scaler[\"scaled\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generator\n",
    "netG = nn.Sequential()\n",
    "with netG.name_scope():\n",
    "    netG.add(nn.Dense(batch_size))\n",
    "    netG.add(nn.BatchNorm(momentum = 0.8))\n",
    "    netG.add(nn.Dropout(0.5))\n",
    "    netG.add(nn.Dense(batch_size+15))\n",
    "    netG.add(nn.BatchNorm(momentum = 0.8))\n",
    "    netG.add(nn.Dropout(0.5))\n",
    "    netG.add(nn.Dense(2*(batch_size+15)))\n",
    "    netG.add(nn.BatchNorm(momentum = 0.8))\n",
    "    netG.add(nn.Dropout(0.5))\n",
    "    netG.add(nn.Dense(1, activation = \"tanh\"))\n",
    "\n",
    "#Discriminator\n",
    "netD = nn.Sequential()\n",
    "with netD.name_scope():\n",
    "    netD.add(nn.Dense(batch_size, activation = \"tanh\"))\n",
    "    netD.add(nn.Dense(15, activation = \"tanh\"))\n",
    "    netD.add(nn.Dense(10, activation='tanh'))\n",
    "    netD.add(nn.Dense(10, activation='tanh'))\n",
    "    netD.add(nn.Dense(5, activation='tanh'))\n",
    "    netD.add(nn.Dense(batch_size))\n",
    "    \n",
    "# print(netG)\n",
    "# print(netD)    \n",
    "\n",
    "# loss\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "#initialize the generator and the discriminator\n",
    "#assigning weights from Normal distribution \n",
    "netG.initialize(mx.init.Normal(0.02), ctx=ctx)\n",
    "netD.initialize(mx.init.Normal(0.02), ctx=ctx)\n",
    "\n",
    "#trainer for the generator and the discriminator\n",
    "#appling an Optimazer to a set of parmas, takes the params of netG/D, and uses adam opt\n",
    "trainerG = gluon.Trainer(netG.collect_params(), 'adam', {'learning_rate': 0.01})\n",
    "trainerD = gluon.Trainer(netD.collect_params(), 'adam', {'learning_rate': 0.05})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ts shape (x, 1)\n",
    "# set up logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "def train_loop(ts, batch_size, epochs):\n",
    "    Y = nd.ones(shape=(ts.shape[0], 1))\n",
    "    train_data = mx.io.NDArrayIter(ts, Y, batch_size, shuffle=False)\n",
    "\n",
    "    real_label = mx.nd.ones((batch_size, ), ctx=ctx)\n",
    "    fake_label = mx.nd.zeros((batch_size, ), ctx=ctx)\n",
    "    metric = mx.metric.Accuracy()\n",
    "\n",
    "    stamp =  datetime.now().strftime('%Y_%m_%d-%H_%M')\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    \n",
    "    g_loss = []\n",
    "    d_loss = []\n",
    "    mse = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        tic = time.time()\n",
    "        train_data.reset()\n",
    "        iter = 0\n",
    "        for i, batch in enumerate(train_data):\n",
    "            # (1) Updating D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            data = batch.data[0].as_in_context(ctx)\n",
    "            noise = mx.nd.array(ts)\n",
    "            noise = noise[(i*batch_size):((i+1)*batch_size), 0]\n",
    "\n",
    "            with autograd.record():\n",
    "                real_output = netD(data)\n",
    "                errD_real = loss(real_output, real_label)\n",
    "\n",
    "                fake = netG(noise)\n",
    "                fake_output = netD(fake.detach())\n",
    "                errD_fake = loss(fake_output, fake_label)\n",
    "                errD = errD_real + errD_fake\n",
    "                errD.backward()\n",
    "\n",
    "            trainerD.step(batch_size)\n",
    "            metric.update([real_label,], [real_output,])\n",
    "            metric.update([fake_label,], [fake_output,])\n",
    "            \n",
    "            #Updating G network: maximize log(D(G(z)))\n",
    "            with autograd.record():\n",
    "                output = netD(fake)\n",
    "                errG = loss(output, real_label)\n",
    "                errG.backward()\n",
    "\n",
    "            trainerG.step(batch_size)\n",
    "\n",
    "        name, acc = metric.get()\n",
    "        metric.reset()\n",
    "        print('\\nbinary training acc at epoch %d: %s=%f' % (epoch, name, acc))\n",
    "        print('time: %f' % (time.time() - tic))\n",
    "\n",
    "        if iter % 1024 == 0:\n",
    "            logging.info('discriminator loss = %f, generator loss = %f, at iter %d epoch %d' \n",
    "                 %(nd.mean(errD).asscalar(), \n",
    "                   nd.mean(errG).asscalar(), iter, epoch))\n",
    "            d_loss.append(nd.mean(errD).asscalar())\n",
    "            g_loss.append(nd.mean(errG).asscalar())\n",
    "        iter = iter + 1\n",
    "\n",
    "        noise = mx.nd.array(ts)\n",
    "\n",
    "        noise = noise[:, 0]\n",
    "        fake = netG(noise)\n",
    "        plt.plot(ts[:200, ], color = \"#007d92\", label = \"Train\")\n",
    "        plt.plot(fake[:200, 0].asnumpy(), color = \"#cd8fa3\", label = \"Generated\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.legend(loc = \"upper left\")\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Error Rate: \" + str(mean_squared_error(ts, fake.asnumpy())))\n",
    "        mse.append(mean_squared_error(ts, fake.asnumpy()))\n",
    "    \n",
    "    #plotting behovior\n",
    "    plt.plot(d_loss, color = \"#cd8fa3\", label = \"Discriminator Loss \")\n",
    "    plt.legend(loc = \"upper right\")\n",
    "    plt.xlabel(\"Number of Epochs\")\n",
    "    plt.ylabel(\"Discriminator Loss\")\n",
    "    plt.title(\"Behavior of Discriminator Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(g_loss, color = \"#f3166b\", label = \"Generator Loss\")\n",
    "    plt.legend(loc = \"upper right\")\n",
    "    plt.xlabel(\"Number of Epochs\")\n",
    "    plt.ylabel(\"Generator Loss\")\n",
    "    plt.title(\"Behavior of Generator Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(g_loss, color = \"#007d92\", label = \"MSE\")\n",
    "    plt.legend(loc = \"upper right\")\n",
    "    plt.xlabel(\"Number of Epochs\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.title(\"Behavior of MSE\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loop(ts_train, batch_size, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#last part of train\n",
    "noise = mx.nd.array(ts_train[ts_train.shape[0]-batch_size:])\n",
    "fake = netG(noise)\n",
    "fake = fake.asnumpy()\n",
    "plt.plot(ts_test[:, ], color = \"#007d92\", label = \"Test\")\n",
    "plt.plot(fake[:, ],  color = \"#cd8fa3\", label = \"Generated\")\n",
    "plt.legend(loc = \"upper left\")\n",
    "plt.show()\n",
    "print(mean_squared_error(ts_test[:batch_size, 0], fake[:, 0]))\n",
    "\n",
    "np.savetxt((\"Version_B//Stationary//\" + str(batch_size) + \"-\" + \"00\" + \".csv\"), fake[:, 0], delimiter=\",\", header=\"Data\")\n",
    "\n",
    "for i in range(int(ts_test.shape[0]/batch_size)-1):\n",
    "    noise = mx.nd.array(ts_test)\n",
    "    noise = noise[batch_size * (i) :batch_size * (1 + i), 0]\n",
    "    fake = netG(noise)\n",
    "    fake = fake.asnumpy()\n",
    "\n",
    "    for x in range(batch_size * (1+i)):\n",
    "        fake = np.insert(fake, x, None, axis=0)\n",
    "    #plot\n",
    "    plt.plot(ts_test[:, ], color = \"#007d92\", label = \"Test\")\n",
    "    plt.plot(fake[:, ],  color = \"#cd8fa3\", label = \"Generated\")\n",
    "    plt.legend(loc = \"upper left\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(mean_squared_error(ts_test[batch_size * (i+1):batch_size * (2 + i), 0], fake[batch_size * (i+1):, 0]))\n",
    "\n",
    "    np.savetxt((\"Version_B//Stationary//\" + str(batch_size) + \"-\" + str(i) + \".csv\"), fake[:, 0], delimiter=\",\", header=\"Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_all = []\n",
    "f = pd.read_csv(\"Version_B//Stationary//\" + str(batch_size) + \"-00\" + \".csv\")\n",
    "f = f[\"# Data\"].values\n",
    "for x in range(f.shape[0]):\n",
    "    fake_all.append(f[x])\n",
    "\n",
    "fake_all\n",
    "\n",
    "for i in range(0, int(ts_test.shape[0]/batch_size)-1):\n",
    "    f = pd.read_csv(\"Version_B//Stationary//\" + str(batch_size) + \"-\" + str(i) + \".csv\")\n",
    "    f = f[\"# Data\"].dropna().values\n",
    "    for x in range(f.shape[0]):\n",
    "        fake_all.append(f[x])\n",
    "\n",
    "fake_all = np.array(fake_all)\n",
    "fake_all = fake_all.reshape(fake_all.shape[0], 1)\n",
    "plt.plot(ts_test[:200, 0], color = \"#007d92\", label = \"Test\")\n",
    "plt.plot(fake_all[:200, 0],  color = \"#cd8fa3\", label = \"Generated\")\n",
    "plt.legend(loc = \"upper left\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.show()\n",
    "\n",
    "print(mean_squared_error(ts_test, fake_all))\n",
    "\n",
    "np.savetxt((\"Version_B//Stationary//\" + \"fake_all-\" + str(batch_size) + \".csv\"), fake_all[:, 0], delimiter=\",\", header = 'Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing the Trendy Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = []\n",
    "\n",
    "generated_inverese_scaled = ts_detrended_test_scaler[\"scaler\"].inverse_transform(fake_all)\n",
    "ts_test_original = ts_scaled[ts_scaled.shape[0] - ts_test.shape[0]:]\n",
    "with_trend = generated_inverese_scaled + linear_reg(detrended_train[\"trend\"].reshape(detrended_train[\"trend\"].shape[0], ), detrended_train[\"trend\"].shape[0], detrended_train[\"trend\"].shape[0] + detrended_test[\"trend\"].shape[0])\n",
    "\n",
    "gen_rescaled = ts_scaler[\"scaler\"].inverse_transform(generated_inverese_scaled)\n",
    "ts_test_rescaled = ts_scaler[\"scaler\"].inverse_transform(ts_test_original)\n",
    "with_trend_rescaled = ts_scaler[\"scaler\"].inverse_transform(with_trend)\n",
    "\n",
    "ts_test_original_list = ts_test_rescaled[:, ].reshape(ts_test_rescaled.shape[0], )\n",
    "with_trend_list = with_trend_rescaled[:, ].reshape(with_trend_rescaled.shape[0], )\n",
    "\n",
    "for u in range(len(ts_test_original_list[:, ])):\n",
    "    diff.append(abs(ts_test_original_list[u]-with_trend_list[u]))\n",
    "\n",
    "residual_stdev = st.stdev(diff)\n",
    "ub = with_trend_rescaled[:, 0] + (1.96*residual_stdev)\n",
    "lb = with_trend_rescaled[:, 0] - (1.96*residual_stdev)\n",
    "\n",
    "plt.plot(ts_test_rescaled[:50], color = \"#007d92\", label = \"Test\")\n",
    "#plt.plot(with_trend_rescaled[:50], color = \"#cd8fa3\", label = \"Generated\")\n",
    "plt.fill_between(x = df.index[:50], y1 = ub[:50, ], \n",
    "                 y2 = lb[:50, ], color = \"gainsboro\",\n",
    "                 label = \"Confidence Bounds\") \n",
    "plt.legend(loc = \"upper left\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "fake_all = pd.read_csv(\"Version_B//Stationary//fake_all-\" + str(batch_size) + \".csv\")\n",
    "fake_all = fake_all[\"# Data\"].values.reshape(fake_all.shape[0], 1)\n",
    "\n",
    "diff = []\n",
    "anomaly_indices = []\n",
    "\n",
    "# test_smoothed[100] = 1.5\n",
    "# test_smoothed[110] = 2\n",
    "# test_smoothed[150] = 2\n",
    "\n",
    "ts_test_rescaled = ts_scaler[\"scaler\"].inverse_transform(ts_test[:, ])[:, 0]\n",
    "fake_rescaled = ts_scaler[\"scaler\"].inverse_transform(fake_all[:, ])\n",
    "\n",
    "ts_test_list = ts_test_rescaled[:, ].reshape(ts_test_rescaled.shape[0], )\n",
    "generated_list = fake_rescaled[:, ].reshape(fake_rescaled.shape[0], )\n",
    "\n",
    "for u in range(len(ts_test_list[:, ])):\n",
    "    diff.append(abs(ts_test_list[u]-generated_list[u]))\n",
    "\n",
    "diff_list_df = pd.DataFrame({\"residual\": diff, \n",
    "                            \"predicted\": generated_list})\n",
    "diff_list_df.head()\n",
    "\n",
    "residual_mean = st.mean(diff)\n",
    "residual_stdev = st.stdev(diff)\n",
    "upper_bound = residual_mean + (3*residual_stdev)\n",
    "lower_bound = residual_mean - (3*residual_stdev)\n",
    "plt.scatter(x = generated_list, y = diff, color = \"#cd8fa3\")\n",
    "#plt.plot(diff[:200])\n",
    "abline(0, upper_bound)\n",
    "abline(0, lower_bound)\n",
    "plt.xlabel(\"Generated\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.title(\"Generated vs Residual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescaled CI for the Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_test_rescaled = ts_scaler[\"scaler\"].inverse_transform(ts_test[:, ])[:, 0]\n",
    "ub_rescaled = ts_scaler[\"scaler\"].inverse_transform(fake_all[:, ] + (1.96*residual_stdev) + residual_mean)\n",
    "lb_rescaled = ts_scaler[\"scaler\"].inverse_transform(fake_all[:, ] - (1.96*residual_stdev) + residual_mean)\n",
    "\n",
    "plt.plot(ts_test_rescaled[:200, ], color = \"#007d92\", label = \"Test\")\n",
    "# plt.plot(fake_all[batch_size:200, ] + (1.96*residual_stdev), \"--\", color = \"grey\", label = \"Upper Bound\")\n",
    "# plt.plot(fake_all[batch_size:200, ] - (1.96*residual_stdev), \"--\", color = \"pink\", label = \"Lower Bound\")\n",
    "\n",
    "plt.plot(ts_scaler[\"scaler\"].inverse_transform(fake_all[:200, ])[:, 0],  color = \"#cd8fa3\", label = \"Generated\")\n",
    "\n",
    "# plt.fill_between(x = df.index[:200], y1 = ub_rescaled[:200, 0], \n",
    "#                  y2 = lb_rescaled[:200, 0], color = \"gainsboro\", \n",
    "#                  label = \"Confidence Bounds\")\n",
    "plt.legend(loc = \"upper left\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avergaed Version and Confidence Bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_10_50 = pd.read_csv(\"Version_B//Stationary//fake_all-10.csv\")\n",
    "fake_10_50 = fake_10_50[\"# Data\"].values\n",
    "\n",
    "fake_20_50 = pd.read_csv(\"Version_B//Stationary//fake_all-20.csv\")\n",
    "fake_20_50 = fake_20_50[\"# Data\"].values\n",
    "\n",
    "fake_25_50 = pd.read_csv(\"Version_B//Stationary//fake_all-25.csv\")\n",
    "fake_25_50 = fake_25_50[\"# Data\"].values\n",
    "\n",
    "fake_30_50 = pd.read_csv(\"Version_B//Stationary//fake_all-30.csv\")\n",
    "fake_30_50 = fake_30_50[\"# Data\"].values\n",
    "\n",
    "fake_36_50 = pd.read_csv(\"Version_B//Stationary//fake_all-36.csv\")\n",
    "fake_36_50 = fake_36_50[\"# Data\"].values\n",
    "\n",
    "fake_mean_all = (fake_10_50 +  fake_36_50 + fake_30_50)/3\n",
    "diff = []\n",
    "\n",
    "ts_test_rescaled = ts_scaler[\"scaler\"].inverse_transform(ts_test[:, ])\n",
    "fake_mean_all_rescaled = ts_scaler[\"scaler\"].inverse_transform(fake_mean_all.reshape(fake_mean_all.shape[0], 1)[:, ])\n",
    "\n",
    "ts_test_rescaled_list = ts_test_rescaled[:, ].reshape(ts_test_rescaled.shape[0], )\n",
    "generated_mean_list = fake_mean_all_rescaled[:, ].reshape(ts_test_rescaled.shape[0], )\n",
    "\n",
    "for u in range(len(ts_test_rescaled_list[:, ])):\n",
    "    diff.append(abs(ts_test_rescaled_list[u]-generated_mean_list[u]))\n",
    "\n",
    "diff_list_df = pd.DataFrame({\"residual\": diff, \n",
    "                            \"predicted\": generated_mean_list})\n",
    "\n",
    "residual_mean = st.mean(diff)\n",
    "residual_stdev = st.stdev(diff)\n",
    "\n",
    "ub = fake_mean_all_rescaled + (1.96*residual_stdev)\n",
    "lb = fake_mean_all_rescaled - (1.96*residual_stdev)\n",
    "\n",
    "plt.plot(ts_test_rescaled[0:200, 0], color = \"#007d92\", label = \"Test\")\n",
    "plt.plot(fake_mean_all_rescaled[:200, ],  color = \"#cd8fa3\", label = \"Generated Mean\")\n",
    "\n",
    "# plt.fill_between(x = df.index[:200], y1 = ub[:200, 0], \n",
    "#                  y2 = lb[:200, 0], color = \"gainsboro\",\n",
    "#                  label = \"Confidence Bounds For Mean\")\n",
    "\n",
    "plt.legend(loc = \"upper left\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Test Error Rate: \" + str(mean_squared_error(ts_test, fake_mean_all)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
